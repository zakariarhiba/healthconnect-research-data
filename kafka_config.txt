================================================================================
KAFKA 3-NODE CLUSTER CONFIGURATION
================================================================================

BROKER CONFIGURATION (All 3 Brokers)
================================================================================

# Broker Identity
broker.id=1                                    # Set to 1, 2, 3 for each broker
rack.id=us-east-1a                            # Availability zone

# Network Configuration
listeners=PLAINTEXT://kafka-1:9092           # Broker 1, change 1 for others
advertised.listeners=PLAINTEXT://kafka-1:9092
listener.security.protocol.map=PLAINTEXT:PLAINTEXT

# Log Configuration
log.dir=/var/kafka-logs                       # Data directory
log.segment.bytes=1073741824                  # 1 GB per segment
log.retention.hours=168                       # 7 days retention
log.retention.check.interval.ms=300000        # Check every 5 minutes

# Replication Configuration
num.partitions=3                              # Default partition count
replication.factor=3                          # 3-way replication
min.insync.replicas=2                         # Majority acknowledgment
unclean.leader.election.enable=false          # Prevent data loss

# Performance Configuration
num.network.threads=8                         # I/O threads
num.io.threads=8                              # Processing threads
socket.send.buffer.bytes=102400               # 100 KB send buffer
socket.receive.buffer.bytes=102400            # 100 KB receive buffer
socket.request.max.bytes=104857600            # 100 MB max request

# Compression
compression.type=snappy                       # CPU-efficient compression
lz4.level=4                                   # LZ4 compression level (if used)

# Topic Configuration
delete.topic.enable=true                      # Allow topic deletion
auto.create.topics.enable=false               # Explicit topic creation

# Consumer Configuration
group.initial.rebalance.delay.ms=3000         # Rebalance delay

# Cluster Configuration
bootstrap.servers=kafka-1:9092,kafka-2:9092,kafka-3:9092
zookeeper.connect=zk-1:2181,zk-2:2181,zk-3:2181
zookeeper.connection.timeout.ms=18000

# JVM Configuration
-Xmx2G                                        # Maximum heap
-Xms2G                                        # Initial heap
-XX:+UseG1GC                                  # Garbage collector
-XX:MaxGCPauseMillis=20
-XX:InitiatingHeapOccupancyPercent=35

================================================================================

TOPIC CONFIGURATION: hospital-data
================================================================================

Topic Name: hospital-data
Partitions: 3
Replication Factor: 3
Min Insync Replicas: 2
Retention: 7 days (604800000 milliseconds)
Retention Size: -1 (unlimited)
Compression Type: snappy
Cleanup Policy: delete
Segment Size: 1 GB
Segment Time: 1 day

Topic Creation Command:
kafka-topics --create \
  --bootstrap-server kafka-1:9092,kafka-2:9092,kafka-3:9092 \
  --topic hospital-data \
  --partitions 3 \
  --replication-factor 3 \
  --config min.insync.replicas=2 \
  --config compression.type=snappy \
  --config retention.ms=604800000

Partition Details:
- Partition 0: Leader=kafka-1, Replicas=[kafka-1,kafka-2,kafka-3], Isr=[kafka-1,kafka-2,kafka-3]
- Partition 1: Leader=kafka-2, Replicas=[kafka-2,kafka-3,kafka-1], Isr=[kafka-2,kafka-3,kafka-1]
- Partition 2: Leader=kafka-3, Replicas=[kafka-3,kafka-1,kafka-2], Isr=[kafka-3,kafka-1,kafka-2]

================================================================================

PRODUCER CONFIGURATION
================================================================================

# Connection
bootstrap.servers=kafka-1:9092,kafka-2:9092,kafka-3:9092
acks=all                                      # Wait for all replicas

# Batching
batch.size=16384                              # 16 KB batch
linger.ms=10                                  # Wait up to 10ms to batch
compression.type=snappy

# Reliability
retries=3                                     # Retry failed sends
max.in.flight.requests.per.connection=5       # Maintain order
buffer.memory=33554432                        # 32 MB buffer

# Timeouts
request.timeout.ms=30000                      # 30 seconds
delivery.timeout.ms=120000                    # 120 seconds

Producer Code Example:
topic = "hospital-data"
partition_key = str(patient_id)              # Route by patient_id
properties = {
    "bootstrap.servers": "kafka-1:9092,kafka-2:9092,kafka-3:9092",
    "acks": "all",
    "compression.type": "snappy",
    "batch.size": 16384,
    "linger.ms": 10
}

================================================================================

CONSUMER CONFIGURATION
================================================================================

# Group
group.id=healthconnect-streaming              # Consumer group
group.instance.id=instance-1                  # Static membership

# Offset Management
auto.offset.reset=earliest                    # Start from beginning
enable.auto.commit=false                      # Manual commit
auto.commit.interval.ms=5000

# Session Management
session.timeout.ms=30000
heartbeat.interval.ms=10000
max.poll.interval.ms=300000

# Performance
fetch.min.bytes=1024                          # Minimum fetch size
fetch.max.wait.ms=500                         # Maximum wait time
max.poll.records=500                          # Max records per poll

Consumer Group Status:
- Group ID: healthconnect-streaming
- Members: 3 (one per Spark executor)
- Topics: hospital-data
- Current Offsets: Lag monitored

================================================================================

PERFORMANCE TUNING
================================================================================

Throughput Optimization:
1. Increase num.io.threads to 16 (if CPU allows)
2. Use batch.size=32KB and linger.ms=20
3. Enable compression (snappy preferred)
4. Increase log.segment.bytes to 2GB
5. Use SSD storage for log.dir

Latency Optimization:
1. Set linger.ms=0 (send immediately)
2. Use batch.size=1KB (smaller batches)
3. Set num.network.threads=16
4. Disable compression for low-latency needs
5. Use dedicated network (separate from general traffic)

Memory Usage:
- Single broker: ~2GB heap recommended
- Per partition: ~50-100MB (estimated)
- Total cluster: 6-12GB (3 brokers × 2-4GB each)

Disk Usage:
- SIMULATION_RESULTS.csv: ~6.5 GB (43.2M × 150 bytes)
- Replication factor 3: ~19.5 GB total
- 7-day retention: ~19.5 GB (rotates daily)

================================================================================

MONITORING & HEALTH CHECKS
================================================================================

Key Metrics:
- Under-replicated partitions: Should be 0
- Offline partitions: Should be 0
- Controller: Should be stable (1 broker)
- Consumer lag: Monitor for each group

Monitoring Commands:
# Broker health
kafka-broker-api-versions --bootstrap-server kafka-1:9092

# Topic status
kafka-topics --describe \
  --bootstrap-server kafka-1:9092 \
  --topic hospital-data

# Consumer group lag
kafka-consumer-groups \
  --bootstrap-server kafka-1:9092 \
  --group healthconnect-streaming \
  --describe

# Metrics export (JMX)
export KAFKA_OPTS="-Dcom.sun.management.jmxremote \
  -Dcom.sun.management.jmxremote.port=9999 \
  -Dcom.sun.management.jmxremote.authenticate=false \
  -Dcom.sun.management.jmxremote.ssl=false"

================================================================================

TROUBLESHOOTING
================================================================================

Issue: High latency (>500ms)
Solution:
1. Check broker CPU usage (should be <80%)
2. Verify network latency between brokers
3. Increase socket buffer sizes
4. Check broker logs for GC pauses

Issue: Data loss
Solution:
1. Verify replication.factor=3
2. Check min.insync.replicas=2
3. Ensure acks=all in producer
4. Check broker disk space (must be >10% free)

Issue: Consumer lag increasing
Solution:
1. Increase num.consumer.threads in Spark
2. Reduce batch.size in producer
3. Check consumer processing time
4. Monitor Spark executor resources

================================================================================

KAFKA CLUSTER HEALTH CHECKS
================================================================================

Every 5 minutes:
✓ Broker connectivity (all 3 online)
✓ Under-replicated partitions = 0
✓ Offline partitions = 0
✓ Controller elected and stable
✓ Zookeeper ensemble healthy

Every hour:
✓ Topic retention accurate
✓ Disk space usage <80%
✓ Consumer lag <100 partitions behind
✓ Broker performance metrics

Every day:
✓ Log compaction status
✓ Replication across racks
✓ Producer success rate >99.9%
✓ Consumer group stability

================================================================================

VERSION: Kafka 3.2.0
CLUSTER SIZE: 3 brokers
TEST DATE: January 11, 2026
STATUS: VALIDATED & TESTED
================================================================================
